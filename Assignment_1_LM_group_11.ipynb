{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ce5pQK3bFn_"
      },
      "source": [
        "# Assignment 1\n",
        "In this assignment you will be creating tools for learning and testing language models.\n",
        "The corpora that you will be working with are lists of tweets in 8 different languages that use the Latin script. The data is provided either formatted as CSV or as JSON, for your convenience. The end goal is to write a set of tools that can detect the language of a given tweet.\n",
        "\n",
        "Do make sure all results are uploaded to CSVs (as well as printed to console) for your assignment to be fully graded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwG8v-Ll49KM"
      },
      "source": [
        "*As a preparation for this task, download the data files from the course git repository.\n",
        "\n",
        "The relevant files are under **lm-languages-data-new**:\n",
        "\n",
        "\n",
        "*   en.csv (or the equivalent JSON file)\n",
        "*   es.csv (or the equivalent JSON file)\n",
        "*   fr.csv (or the equivalent JSON file)\n",
        "*   in.csv (or the equivalent JSON file)\n",
        "*   it.csv (or the equivalent JSON file)\n",
        "*   nl.csv (or the equivalent JSON file)\n",
        "*   pt.csv (or the equivalent JSON file)\n",
        "*   tl.csv (or the equivalent JSON file)\n",
        "*   test.csv (or the equivalent JSON file)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrsUz4r5EcaF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import re\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import sys\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xC-87z2GWMq",
        "outputId": "e75a3428-caf5-4ded-8379-c3dbd31b32cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nlp-course'...\n",
            "remote: Enumerating objects: 71, done.\u001b[K\n",
            "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 71 (delta 29), reused 40 (delta 11), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (71/71), 11.28 MiB | 3.65 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/kfirbar/nlp-course.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOVb4IhsqimJ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Important note: please use only the files under lm-languages-data-new and NOT under lm-languages-data**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYdhPfbAGkip",
        "outputId": "85c34d0d-9048-4549-b19b-81701a230302"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en.csv\t es.json  in.csv   it.json  pt.csv    test.json   tl.csv\n",
            "en.json  fr.csv   in.json  nl.csv   pt.json   tests.csv   tl.json\n",
            "es.csv\t fr.json  it.csv   nl.json  test.csv  tests.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!ls nlp-course/lm-languages-data-new\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ashyu_mT28o6"
      },
      "source": [
        "**Part 1**\n",
        "\n",
        "Write a function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data. **Our token definition is a single UTF-8 encoded character**. So, the vocabulary list is a simple Python list of all the characters that you see at least once in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCfzsITW8Yaj"
      },
      "outputs": [],
      "source": [
        "def preprocess(data_dir):\n",
        "    vocab = set()\n",
        "    for filename in os.listdir(data_dir):\n",
        "        if 'csv' in filename and 'test' not in filename:\n",
        "            # Read CSV file with two columns\n",
        "            df = pd.read_csv(data_dir + \"/\" + filename, encoding=\"utf-8\")\n",
        "            for tweet in df['tweet_text']:\n",
        "                for char in tweet:\n",
        "                    if char not in vocab:\n",
        "                        vocab.add(char)\n",
        "    vocab.add('<S>')\n",
        "    vocab.add('<E>')\n",
        "    return list(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb2PGj0Yc2TY"
      },
      "source": [
        "**Part 2**\n",
        "\n",
        "Write a function lm that generates a language model from a textual corpus. The function should return a dictionary (representing a model) where the keys are all the relevant n-1 sequences, and the values are dictionaries with the n_th tokens and their corresponding probabilities to occur. For example, for a trigram model (tokens are characters), it should look something like:\n",
        "\n",
        "{\n",
        "  \"ab\":{\"c\":0.5, \"b\":0.25, \"d\":0.25},\n",
        "  \"ca\":{\"a\":0.2, \"b\":0.7, \"d\":0.1}\n",
        "}\n",
        "\n",
        "which means for example that after the sequence \"ab\", there is a 0.5 chance that \"c\" will appear, 0.25 for \"b\" to appear and 0.25 for \"d\" to appear.\n",
        "\n",
        "Note - You should think how to add the add_one smoothing information to the dictionary and implement it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMC_u8eQbVvZ"
      },
      "outputs": [],
      "source": [
        "def lm(n, vocabulary, data_file_path, add_one):\n",
        "  # n - the n-gram to use (e.g., 1 - unigram, 2 - bigram, etc.)\n",
        "  # vocabulary - the vocabulary list (which you should use for calculating add_one smoothing)\n",
        "  # data_file_path - the data_file from which we record probabilities for our model\n",
        "  # add_one - True/False (use add_one smoothing or not)\n",
        "    # \n",
        "    if add_one:\n",
        "        model = defaultdict(lambda: defaultdict(lambda: 1.0 / len(vocabulary)))\n",
        "        model[\"unk\"][\"unk\"] = 1.0 / len(vocabulary)\n",
        "    else:\n",
        "        model = defaultdict(lambda: defaultdict(int))\n",
        "        model[\"unk\"][\"unk\"] = 0\n",
        "    total_counts = defaultdict(int)\n",
        "    total_counts[\"unk\"] = 1\n",
        "    if 'csv' in data_file_path:\n",
        "        # Read CSV file with two columns\n",
        "        df = pd.read_csv(data_file_path, encoding=\"utf-8\")\n",
        "    for tweet in df['tweet_text']:\n",
        "          tweet = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \"\", tweet)\n",
        "          tweet = re.sub(r'@\\w+\\s*', '', tweet)              \n",
        "          tweet = re.sub('[^\\w\\s]+', '', tweet)\n",
        "          # Pad the text with special start and end tokens\n",
        "          tweet = '<S>'*(n) + tweet + '<E>'\n",
        "          j = 0\n",
        "          #deal with <S> as it is of size 4\n",
        "          for i in range(n):\n",
        "              # Increment the count for this n-gram and token\n",
        "              model[tweet[i+(j):i+3*n]][tweet[i+3*n]] += 1\n",
        "              # Increment the total count for this n-gram\n",
        "              total_counts[tweet[i+(j):i+3*n]] += 1\n",
        "              j += 2\n",
        "          for i in range(3*n,len(tweet)-3-n):\n",
        "              # Increment the count for this n-gram and token\n",
        "              model[tweet[i:i+n]][tweet[i+n]] += 1\n",
        "              # Increment the total count for this n-gram\n",
        "              total_counts[tweet[i:i+n]] += 1\n",
        "          #deal with <E> as it needs to be a token of size 4 and not 1\n",
        "          # Increment the count for this n-gram and token\n",
        "          model[tweet[len(tweet)-n-3:len(tweet)-3]][tweet[len(tweet)-3:]] += 1\n",
        "          # Increment the total count for this n-gram\n",
        "          total_counts[tweet[len(tweet)-n-3:len(tweet)-3]] += 1\n",
        "    # Convert counts to probabilities\n",
        "    for ngram in model.keys():\n",
        "        total = total_counts[ngram]\n",
        " #       for token in vocabulary:\n",
        "        for token in model[ngram].keys():\n",
        "            count = model[ngram][token]\n",
        "            if add_one:\n",
        "                count += 1\n",
        "                total += len(vocabulary)\n",
        "            model[ngram][token] = count / total\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M8TchtI22I3"
      },
      "source": [
        "**Part 3**\n",
        "\n",
        "Write a function *eval* that returns the perplexity of a model (dictionary) running over a given data file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0kkMn328-lJ"
      },
      "outputs": [],
      "source": [
        "def eval(n, model, data_file):\n",
        "    # n - the n-gram that you used to build your model (must be the same number)\n",
        "    # model - the dictionary (model) to use for calculating perplexity\n",
        "    # data_file - the tweets file that you wish to claculate a perplexity score for\n",
        "    if 'csv' in data_file:\n",
        "        # Read CSV file with two columns\n",
        "        df = pd.read_csv(data_file, encoding=\"utf-8\")\n",
        "    probs = []\n",
        "    for tweet in df['tweet_text']:\n",
        "          tweet = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \"\", tweet)\n",
        "          tweet = re.sub(r'@\\w+\\s*', '', tweet) \n",
        "          tweet = re.sub('[^\\w\\s]+', '', tweet)\n",
        "          # Pad the text with special start and end tokens\n",
        "          tweet = '<S>'*(n) + tweet + '<E>'\n",
        "          j = 0\n",
        "          #deal with <S> as it is of size 3\n",
        "          for i in range(n):\n",
        "              temp = 0\n",
        "              ngram = tweet[i+(j):i+3*n]\n",
        "              if ngram in model.keys():\n",
        "                  if tweet[i+3*n] in model[ngram].keys():\n",
        "                      temp = model[ngram][tweet[i+3*n]]\n",
        "                      probs.append(temp)\n",
        "              if temp == 0:\n",
        "                  temp = model[\"unk\"][\"unk\"]\n",
        "                  if temp > 0:\n",
        "                      probs.append(temp)\n",
        "                  # else:\n",
        "                  #     temp = 1.e-17\n",
        "                  #     probs.append(temp)\n",
        "              j += 2\n",
        "          for i in range(3*n,len(tweet)-n-3):\n",
        "              temp = 0\n",
        "              ngram = tweet[i:i+n]\n",
        "              if ngram in model.keys():\n",
        "                  if tweet[i+n] in model[ngram].keys():   \n",
        "                      temp = model[ngram][tweet[i+n]]\n",
        "                      if temp > 0: \n",
        "                          probs.append(temp)\n",
        "              if temp == 0:\n",
        "                  temp = model[\"unk\"][\"unk\"]\n",
        "                  if temp > 0:\n",
        "                      probs.append(temp)\n",
        "                  # else:\n",
        "                  #     temp = 1.e-17\n",
        "                  #     probs.append(temp)\n",
        "\n",
        "          temp = 0\n",
        "          ngram = tweet[len(tweet)-n-3:len(tweet)-3]\n",
        "          if ngram in model.keys():\n",
        "              if tweet[len(tweet)-3:] in model[ngram].keys():   \n",
        "                  temp = model[ngram][tweet[len(tweet)-3:]]\n",
        "                  if temp > 0:\n",
        "                          probs.append(temp)\n",
        "          if temp == 0:\n",
        "              temp = model[\"unk\"][\"unk\"]\n",
        "              if temp > 0:\n",
        "                  probs.append(temp)\n",
        "              # else:\n",
        "              #     temp = 1.e-17\n",
        "              #     probs.append(temp)\n",
        "    \n",
        "    entropy = -np.mean(np.log2(probs))\n",
        "    P = 2** entropy\n",
        "    return P"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enGmtLE3921p"
      },
      "source": [
        "**Part 4**\n",
        "\n",
        "Write a function *match* that creates a model for every relevant language, using a specific value of *n* and *add_one*. Then, calculate the perplexity of all possible pairs (e.g., en model applied on the data files en ,es, fr, in, it, nl, pt, tl; es model applied on the data files en, es...). This function should return a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages. Then, the values are the relevant perplexity values.\n",
        "\n",
        "Save the dataframe to a CSV with the name format: {student_id_1}\\_...\\_{student_id_n}\\_part4.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caAxLE9s_fvn"
      },
      "outputs": [],
      "source": [
        "def match(n, add_one):\n",
        "    # n - the n-gram to use for creating n-gram models\n",
        "    # add_one - use add_one smoothing or not\n",
        "    df = pd.DataFrame(columns=['en', 'es', 'fr', 'in', 'it', 'nl', 'pt', 'tl'], index=['en', 'es', 'fr', 'in', 'it', 'nl', 'pt', 'tl'])\n",
        "    vocabulary = preprocess('nlp-course/lm-languages-data-new/')  \n",
        "    for modelfile in ['en.csv', 'in.csv', 'pt.csv', 'tl.csv', 'fr.csv', 'nl.csv', 'es.csv', 'it.csv']: \n",
        "        model = lm(n, vocabulary, 'nlp-course/lm-languages-data-new/' + modelfile, add_one)\n",
        "        for vocabfile in ['en.csv', 'in.csv', 'pt.csv', 'tl.csv', 'fr.csv', 'nl.csv', 'es.csv', 'it.csv']:\n",
        "            P = eval(n, model, 'nlp-course/lm-languages-data-new/' + vocabfile)\n",
        "            \n",
        "            df.loc[modelfile[:2], vocabfile[:2]] = P\n",
        "\n",
        "    df.to_csv(f\"{208542944}_{318339041}_part4.csv\")\n",
        "    return df\n",
        "# match(4, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZxi_NoFP5Bh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d72ed23-c2b8-40f6-940f-be6a1aa41d32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waGMwA8H_n17"
      },
      "source": [
        "**Part 5**\n",
        "\n",
        "Run match with *n* values 1-4, once with add_one and once without, and print the 8 tables to this notebook, one after another.\n",
        "\n",
        "Load each result to a dataframe and save to a CSV with the name format: \n",
        "\n",
        "for cases with add_one: {student_id_1}\\_...\\_{student_id_n}\\_n1\\_part5.csv\n",
        "\n",
        "For cases without add_one:\n",
        "{student_id_1}\\_...\\_{student_id_n}\\_n1\\_wo\\_addone\\_part5.csv\n",
        "\n",
        "Follow the same format for n2,n3, and n4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nk32naXyAMdl"
      },
      "outputs": [],
      "source": [
        "def run_match():\n",
        "    for n in range(1,5):\n",
        "       match(n, True).to_csv(f\"{208542944}_{318339041}_{n}_part5.csv\")\n",
        "       match(n, False).to_csv(f\"{208542944}_{318339041}_{n}_no_addone_part5.csv\")\n",
        "run_match()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg4h5Cl0q2nR"
      },
      "source": [
        "**Part 6**\n",
        "\n",
        "Each line in the file test.csv contains a sentence and the language it belongs to. Write a function that uses your language models to classify the correct language of each sentence.\n",
        "\n",
        "Important note regarding the grading of this section: this is an open question, where a different solution will yield different accuracy scores. any solution that is not trivial (e.g. returning 'en' in all cases) will be accepted. We do reserve the right to give bonus points to exceptionally good/creative solutions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qD6IRIQLrlZF"
      },
      "outputs": [],
      "source": [
        "def classify(n,add_one):\n",
        "      \n",
        "      # Read CSV file with two columns\n",
        "      df = pd.read_csv('nlp-course/lm-languages-data-new/test.csv', encoding=\"utf-8\")\n",
        "      ans = defaultdict(lambda: defaultdict(int))\n",
        "      labels = np.empty((len(df), 2), dtype=object)\n",
        "      vocabulary = preprocess('nlp-course/lm-languages-data-new')\n",
        "      eval = 0\n",
        "      for modelfile in ['en.csv', 'in.csv', 'pt.csv', 'tl.csv', 'fr.csv', 'nl.csv', 'es.csv', 'it.csv']:   \n",
        "          model = lm(n, vocabulary, 'nlp-course/lm-languages-data-new/' + modelfile, add_one)\n",
        "          for iter,tweet in df.iterrows():\n",
        "              if iter > 0:      \n",
        "                    labels[iter][0] = tweet['label']\n",
        "                    vals = []\n",
        "                    tweet = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \"\", tweet[\"tweet_text\"])\n",
        "                    tweet = re.sub(r'@\\w+\\s*', '', tweet)   \n",
        "                    tweet = re.sub('[^\\w\\s]+', '', tweet)\n",
        "                    # Pad the text with special start and end tokens\n",
        "                    tweet = '<S>'*(n) + tweet + '<E>'\n",
        "                    j = 0\n",
        "                    for i in range(n):\n",
        "                        temp = 0\n",
        "                        ngram = tweet[i+(j):i+3*n]\n",
        "                        if ngram in model.keys():\n",
        "                            if tweet[i+3*n] in model[ngram].keys():   \n",
        "                                temp = model[ngram][tweet[i+3*n]]\n",
        "                                if temp > 0:\n",
        "                                    vals.append(temp)\n",
        "                        if temp == 0:\n",
        "                            temp = model[\"unk\"][\"unk\"]\n",
        "                            if temp > 0:\n",
        "                                vals.append(temp)\n",
        "                            # else:\n",
        "                            #     vals.append(1.e-17)\n",
        "\n",
        "                        j += 2\n",
        "                    for i in range(3*n,len(tweet)-n-3):\n",
        "                        temp = 0\n",
        "                        ngram = tweet[i:i+n]\n",
        "                        if ngram in model.keys():\n",
        "                            if tweet[i+n] in model[ngram].keys():   \n",
        "                                temp = model[ngram][tweet[i+n]]\n",
        "                                if temp > 0:\n",
        "                                    vals.append(temp)\n",
        "\n",
        "                        if temp == 0:\n",
        "                            temp = model[\"unk\"][\"unk\"]\n",
        "                            if temp > 0:\n",
        "                                vals.append(temp)\n",
        "                            # else:\n",
        "                            #     vals.append(1.e-17)\n",
        "                    #deal with <E> as it needs to be a token of size 4 and not 1\n",
        "                    temp = 0\n",
        "                    ngram = tweet[len(tweet)-n-3:len(tweet)-3]\n",
        "                    if ngram in model.keys():\n",
        "                        if tweet[len(tweet)-3:] in model[ngram].keys():   \n",
        "                            temp = model[ngram][tweet[len(tweet)-3:]]\n",
        "                        if temp > 0:\n",
        "                                vals.append(temp)\n",
        "                    if temp == 0:\n",
        "                        temp = model[\"unk\"][\"unk\"]\n",
        "                        if temp > 0:\n",
        "                            vals.append(temp)\n",
        "                        # else:\n",
        "                        #     vals.append(1.e-17)\n",
        "                    entropy = 0\n",
        "                    # for ngram in H.keys():\n",
        "                    #     entropy -= math.log2(H[ngram])\n",
        "                    if vals:   \n",
        "                        entropy = -np.mean(np.log2(vals))\n",
        "                        P = 2** entropy\n",
        "\n",
        "                    ans[iter][modelfile] = P\n",
        "      for i in range(1,len(df)):\n",
        "          guess = min(ans[i],key=ans[i].get)\n",
        "          labels[i][1] = guess.split(\".\")[0]\n",
        "      return labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clasification_result = classify(2, True)"
      ],
      "metadata": {
        "id": "QOr57OgImSnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clasification_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwogLL7Sm7lv",
        "outputId": "019c3bb7-a600-43ce-f2f7-f0d393d4c433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[None, None],\n",
              "       ['it', 'it'],\n",
              "       ['tl', 'in'],\n",
              "       ...,\n",
              "       ['it', 'it'],\n",
              "       ['pt', 'pt'],\n",
              "       ['en', 'en']], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5ECmLd3rktZ"
      },
      "source": [
        "**Part 7**\n",
        "\n",
        "Calculate the F1 score of your output from part 6. (hint: you can use https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html). \n",
        "\n",
        "Load the results to a CSV (using a DataFrame), where the row indicates the F1 results, and the columns indicate the model used. Name it {student_id_1}\\_...\\_{student_id_n}\\_part7.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOBO3YQls66r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77705c80-2d27-4991-d199-3aca6e40f0cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the score for model_1_True is: 0.8730725237412568 where n = 1 and add one is: True\n",
            "the score for model_1_False is: 0.8639926511708674 where n = 1 and add one is: False\n",
            "the score for model_2_True is: 0.9250610494684947 where n = 2 and add one is: True\n",
            "the score for model_2_False is: 0.9112248198769488 where n = 2 and add one is: False\n",
            "the score for model_3_True is: 0.9232825156550549 where n = 3 and add one is: True\n",
            "the score for model_3_False is: 0.8476553626536604 where n = 3 and add one is: False\n",
            "max prob: 0.9250610494684947 where n = 2 and add one is: 1\n"
          ]
        }
      ],
      "source": [
        "def calc_f1(result):\n",
        "  return f1_score(result[1:, 0], result[1:, 1], average='macro')\n",
        "arg_max = 0\n",
        "current_n = 0 \n",
        "is_True = -1\n",
        "df = pd.DataFrame(columns=['model', 'f1_score'], index=[1, 2, 3, 4, 5, 6, 7, 8])\n",
        "for i in range(1,4):\n",
        "    c1  = calc_f1(classify(i,True))\n",
        "    print(f\"the score for model_{i}_True is:\",c1,\"where n =\",i ,\"and add one is:\", \"True\")\n",
        "    if c1 > arg_max:\n",
        "        arg_max = c1\n",
        "        current_n = i\n",
        "        is_True = 1\n",
        "    c2  = calc_f1(classify(i,False))\n",
        "    print(f\"the score for model_{i}_False is:\",c2,\"where n =\",i ,\"and add one is:\", \"False\")\n",
        "    if c2 > arg_max:\n",
        "        arg_max = c2\n",
        "        current_n = i\n",
        "        is_True = 0\n",
        "    df.loc[i]['model'] = f\"model_{i}_True\"\n",
        "    df.loc[i+4]['model'] = f\"model_{i}_False\"\n",
        "    df.loc[i]['f1_score'] = c1\n",
        "    df.loc[i+4]['f1_score'] = c2\n",
        "df.to_csv(f\"{208542944}_{318339041}_part7.csv\")\n",
        "print(\"max prob:\",arg_max,\"where n =\",current_n,\"and add one is:\", is_True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfBYgfjADNPL"
      },
      "source": [
        "<br><br><br><br>\n",
        "**Part 8**  \n",
        "Let's use your Language model (dictionary) for generation (NLG).\n",
        "\n",
        "When it comes to sampling from a language model decoder during text generation, there are several different methods that can be used to control the randomness and diversity of the generated text. \n",
        "\n",
        "Some of the most commonly used methods include:\n",
        "\n",
        "> `Greedy sampling`\n",
        "In this method, the model simply selects the word with the highest probability as the next word at each time step. This method can produce fluent text, but it can also lead to repetitive or predictable output.\n",
        "\n",
        "> `Temperature scaling`  \n",
        "Temperature scaling involves scaling the logits output of the language model by a temperature parameter before softmax normalization. This has the effect of smoothing the distribution of probabilities and increasing the probability of lower-probability words, which can lead to more diverse and creative output.\n",
        "\n",
        "> `Top-K sampling`  \n",
        "In this method, the model restricts the sampling to the top-K most likely words at each time step, where K is a predefined hyperparameter. This can generate more diverse output than greedy sampling, while limiting the number of low-probability words that are sampled.\n",
        "\n",
        "> `Nucleus sampling` (also known as top-p sampling)  \n",
        "This method restricts the sampling to the smallest possible set of words whose cumulative probability exceeds a certain threshold, defined by a hyperparameter p. Like top-K sampling, this can generate more diverse output than greedy sampling, while avoiding sampling extremely low probability words.\n",
        "\n",
        "> `Beam search`  \n",
        "Beam search involves maintaining a fixed number k of candidate output sequences at each time step, and then selecting the k most likely sequences based on their probabilities. This can improve the fluency and coherence of the output, but may not produce as much diversity as sampling methods.\n",
        "\n",
        "The choice of sampling method depends on the specific application and desired balance between fluency, diversity, and randomness. Hyperparameters such as temperature, K, p, and beam size can also be tuned to adjust the behavior of the language model during sampling.\n",
        "\n",
        "\n",
        "You may read more about this concept in <a href='https://huggingface.co/blog/how-to-generate#:~:text=pad_token_id%3Dtokenizer.eos_token_id)-,Greedy%20Search,-Greedy%20search%20simply'>this</a> blog post.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbReeHtwNWKS"
      },
      "source": [
        "**Please added the needed code for each sampeling method:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4TrLs1kI3fW"
      },
      "outputs": [],
      "source": [
        "def sample_greedy(probabilities, k=1):\n",
        "    return max(probabilities,key=probabilities.get)\n",
        "\n",
        "\n",
        "def sample_temperature(probabilities, temperature=1.0, k=1):\n",
        "    probs = np.array(list(probabilities.values()))\n",
        "    scaled_logits = np.log(probs) / temperature\n",
        "    scaled_probs = np.exp(scaled_logits) / np.sum(np.exp(scaled_logits))\n",
        "    return np.random.choice(list(probabilities.keys()), p=scaled_probs)\n",
        "\n",
        "\n",
        "def sample_topK(probabilities, k=2,is_char=False):\n",
        "    # Sort the dictionary by value in descending order and get the top k items\n",
        "    sorted_dict = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)[:k]\n",
        "    # Extract the keys of the top k items and return them in a list\n",
        "    top_k = [(key,value) for key, value in sorted_dict]\n",
        "    if is_char:\n",
        "        return top_k\n",
        "    return random.choice(top_k)[0] \n",
        "\n",
        "\n",
        "def sample_topP(probabilities, p=0.9):\n",
        "    sorted_probs = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
        "    cumulative_sum = 0\n",
        "    result = []\n",
        "    for (key, value) in sorted_probs:\n",
        "        if cumulative_sum < p:\n",
        "            cumulative_sum += value\n",
        "            result.append(key)\n",
        "        else:\n",
        "            break\n",
        "    return random.choice(result)\n",
        "\n",
        "\n",
        "def sample_beam(n,start_token, model, k=3, target_len=10):\n",
        "    currnt_candidates = [()]\n",
        "    all_candits = []\n",
        "    best_k_candidates = [()]\n",
        "    len_start = len(start_token)\n",
        "    start_token = '<S>'*(n-len_start) + start_token\n",
        "    ngram = start_token[:3*n]\n",
        "    best_k_candidates = sample_topK(model[ngram], k=k,is_char=True)\n",
        "    for index in range(1,int(target_len)-1):\n",
        "          for ngram_candit in best_k_candidates:\n",
        "                current_val = ngram_candit[1]\n",
        "                current_string = start_token + ngram_candit[0]\n",
        "       \n",
        "                if index < n+1-len_start:\n",
        "                    ngram = current_string[index+2*index:index+3*n]\n",
        "                else :\n",
        "                    ngram = current_string[-n:]\n",
        "                currnt_candidates = sample_topK(model[ngram], k=k,is_char=True)\n",
        "                for candidat in currnt_candidates:\n",
        "                      all_candits.append((ngram_candit[0]+candidat[0], ngram_candit[1]+candidat[1]))\n",
        "          best_k_candidates = sorted(all_candits, key=lambda tup: tup[1], reverse=True)[:k]\n",
        "    return best_k_candidates\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Giylo6-lI21t"
      },
      "source": [
        "Use your Language Model to generate each one out of the following examples with the coresponding params.    \n",
        "Notice the 4 core issues: \n",
        "- Starting tokens\n",
        "- Length of the generation\n",
        "- Sampling methond (use all)\n",
        "- Stop Token (if this token is sampled, stop generating)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pUOkRtjN1mI"
      },
      "outputs": [],
      "source": [
        "test_ = {\n",
        "    'example1' : {\n",
        "        'start_tokens' : \"H\",\n",
        "        'sampling_method' : ['greedy','beam'],\n",
        "        'gen_length' : \"10\",\n",
        "        'stop_token' : \"\\n\",\n",
        "        'generation' : []\n",
        "    },\n",
        "    'example2' : {\n",
        "        'start_tokens' : \"H\",\n",
        "        'sampling_method' : ['temperature','topK','topP'],\n",
        "        'gen_length' : \"10\",\n",
        "        'stop_token' : \"\\n\",\n",
        "        'generation' : []\n",
        "    },\n",
        "    'example3' : {\n",
        "        'start_tokens' : \"He\",\n",
        "        'sampling_method' : ['greedy','beam','temperature','topK','topP'],\n",
        "        'gen_length' : \"20\",\n",
        "        'stop_token' : \"me\",\n",
        "        'generation' : []\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTbF-9zKVchQ"
      },
      "source": [
        "Use your LM to generate a string based on the parametes of each examples, and store the generation sequance at the generation list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zf-omUXQezz"
      },
      "outputs": [],
      "source": [
        "### your code here ###\n",
        "n = 4\n",
        "add_one = False\n",
        "vocabulary = preprocess('nlp-course/lm-languages-data-new/') \n",
        "model = lm(n, vocabulary, 'nlp-course/lm-languages-data-new/en.csv', add_one)\n",
        "for exmp in test_.values():\n",
        "    gen_length = int(exmp[\"gen_length\"])\n",
        "    stop_token = exmp[\"stop_token\"]\n",
        "    for func in exmp[\"sampling_method\"]:\n",
        "        start_token = exmp[\"start_tokens\"]\n",
        "        start_len = len(start_token)\n",
        "        ganarted_text = \"\"\n",
        "        if func == 'beam':\n",
        "            ans = sample_beam(n, start_token, model, k=3, target_len=gen_length)\n",
        "            ganarted_text += ans[0][0]\n",
        "        else:\n",
        "            # ganarted_text += start_token\n",
        "            start_token = '<S>'*(n-start_len) + start_token\n",
        "            for index in range(int(gen_length)-1):\n",
        "                if index < n+1-start_len:\n",
        "                    ngram = start_token[index+2*index:index+3*n]\n",
        "                else :\n",
        "                    ngram = start_token[-n:]\n",
        "                probabilities = model[ngram]\n",
        "                if func == 'greedy':\n",
        "                    next_char = sample_greedy(probabilities)\n",
        "                elif func == 'temperature':\n",
        "                    next_char = sample_temperature(probabilities)\n",
        "                elif func == 'topK':\n",
        "                    next_char = sample_topK(probabilities)\n",
        "                elif func == 'topP':\n",
        "                    next_char = sample_topP(probabilities)\n",
        "                else : \n",
        "                    break\n",
        "                if next_char == \"<E>\":\n",
        "                    break\n",
        "                ganarted_text += next_char\n",
        "                start_token += next_char\n",
        "        ganarted_text += stop_token\n",
        "        exmp[\"generation\"].append(ganarted_text)\n",
        "\n",
        "\n",
        "\n",
        "#####################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvla30-lVw8n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4903a4a8-35ac-4524-ff57-3ae865e942b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------- NLG --------\n",
            "example1:\n",
            "\tgreedy >> Hello Bad This is a great a big book at the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to t\n",
            "\n",
            "\tbeam >> Hey Republicans dont getting in the best for you are bill be appreciation days been this is that you are bill be appreciation days been this is that you are bill be appreciation days been this is that you are bill be appreciation days been this is that you are bill be appreciation days been this is that you are bill be appreciation days been this is that you are bill be appreciation days been this is that you are bill be appreciation days been this is that you are bill be appreciation days been this is that you are bill be appreciation days been this is that you are bill be appreciation days been this is that you are bill be appreciation days been this is that you are bill be appreciation days been this is that you are bill be appreciation days been this is that you are bill be appreciation days been this is that you are bill be appreciation days been this is that you are bill be appreciation days been this is that you are bill be appreciation days been this is that you are bill be appr\n",
            "\n",
            "\n",
            "example2:\n",
            "\ttemperature >> Hacker our\n",
            "\n",
            "\ttopK >> Howare thi\n",
            "\n",
            "\ttopP >> Had to  ma\n",
            "\n",
            "\n",
            "example3:\n",
            "\tgreedy >> Hello Bad This is a gme\n",
            "\tbeam >> Hey Republicans dont me\n",
            "\ttemperature >> Hellowed overnment isme\n",
            "\ttopK >> Hey RedNose of a roomme\n",
            "\ttopP >> Health plane  Cup 240me\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### do not change ###\n",
        "print('-------- NLG --------')\n",
        "\n",
        "for k,v in test_.items():\n",
        "  l = ''.join([f'\\t{sm} >> {v[\"start_tokens\"]}{g}\\n' for sm,g in zip(v['sampling_method'],v['generation'])])\n",
        "  print(f'{k}:')\n",
        "  print(l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEtckSWNANqW"
      },
      "source": [
        "<br><br><br>\n",
        "# **Good luck!**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}